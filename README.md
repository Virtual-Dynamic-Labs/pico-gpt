# ğŸ¤– Pico-GPT

A minimal, educational implementation of the GPT (Generative Pre-trained Transformer) architecture in PyTorch with clean, professional project structure. This project provides a complete, well-documented codebase for understanding how GPT models work under the hood.

## ğŸŒŸ Features

- **Complete GPT Architecture**: Multi-head self-attention, position embeddings, layer normalization
- **Professional Structure**: Organized by purpose with src/, training/, cli/, tests/
- **Multiple Interfaces**: Interactive CLI, direct generation, Python API
- **Optimized Training**: Ultra-fast conversation training (16 seconds)
- **GPU Acceleration**: Automatic CUDA detection and usage
- **Configurable Models**: Easily adjust layers, heads, embedding dimensions
- **Text Generation**: Autoregressive generation with temperature and top-k sampling
- **Educational Examples**: Comprehensive examples and documentation

## ğŸ“‹ Requirements

- Python 3.7+
- PyTorch 1.12.0+
- NumPy 1.21.0+
- Regex 2022.1.18+

## ğŸ“ Project Structure

```
pico-gpt/
â”œâ”€â”€ ğŸ“ src/                      # Core implementation
â”‚   â”œâ”€â”€ pico_gpt.py             # Main GPT model & architecture
â”‚   â”œâ”€â”€ tokenizer.py            # Simple & BPE tokenizers
â”‚   â”œâ”€â”€ fast_tokenizer.py       # Optimized GPT-2 style tokenizer
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ training/                 # Training scripts
â”‚   â””â”€â”€ train_conversation.py   # ğŸŒŸ BEST: Conversation model training
â”‚
â”œâ”€â”€ ğŸ“ cli/                      # User interfaces
â”‚   â”œâ”€â”€ cli_client.py           # ğŸŒŸ MAIN: Interactive chat CLI
â”‚   â””â”€â”€ generate.py             # Simple text generation
â”‚
â”œâ”€â”€ ğŸ“ models/                   # Trained models
â”‚   â””â”€â”€ pico_gpt_conversation.pt # ğŸŒŸ Conversation model (26.2M params)
â”‚
â”œâ”€â”€ ğŸ“ datasets/                 # Training data & tokenizers
â”‚   â”œâ”€â”€ clean_conversation_data.txt      # ğŸŒŸ Clean chat data
â”‚   â”œâ”€â”€ fast_tokenizer_gpt2_8000.pkl    # ğŸŒŸ Optimized tokenizer
â”‚   â”œâ”€â”€ combined_enhanced_data.txt       # Enhanced training data
â”‚   â”œâ”€â”€ comprehensive_conversations.txt  # Comprehensive dialogue data
â”‚   â”œâ”€â”€ conversation_training.txt        # Core training conversations
â”‚   â”œâ”€â”€ smart_reasoning_data.txt         # Advanced reasoning examples
â”‚   â””â”€â”€ [other datasets...]
â”‚
â”œâ”€â”€ ğŸ“ tests/                    # Test & example scripts
â”‚   â”œâ”€â”€ example.py              # Basic functionality demo
â”‚   â”œâ”€â”€ test_conversation.py    # Conversation testing
â”‚   â”œâ”€â”€ debug_conversation.py   # Debugging tools
â”‚   â””â”€â”€ test_train.py           # Training verification
â”‚
â”œâ”€â”€ ğŸ“ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ create_clean_conversation_data.py  # Data preprocessing
â”‚   â”œâ”€â”€ create_conversation_data.py        # Conversation generation
â”‚   â”œâ”€â”€ create_smart_dataset.py           # Smart dataset creation
â”‚   â”œâ”€â”€ download_dataset.py               # Dataset downloading
â”‚   â”œâ”€â”€ main.py                           # Main entry point
â”‚   â””â”€â”€ run.py                            # Simple runner
â”‚
â”œâ”€â”€ ğŸ“ benchmarks/               # Performance testing
â”‚   â”œâ”€â”€ benchmark_cuda_vs_cpu.py      # CUDA vs CPU benchmarking
â”‚   â”œâ”€â”€ benchmark_large_model.py      # Large model performance
â”‚   â””â”€â”€ test_large_model.py           # Large model testing
â”‚
â”œâ”€â”€ ğŸ“„ setup.py                  # Package configuration
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ upload_to_hf.py           # Hugging Face upload script
â”œâ”€â”€ ğŸ“„ run_cli.ps1               # Windows PowerShell launcher
â””â”€â”€ ğŸ“„ README.md                 # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd pico-gpt

# Install dependencies
pip install -r requirements.txt
```

### 2. Interactive Chat (Recommended)

```bash
# Using main scripts
python scripts/run.py
python scripts/main.py

# Using CLI directly
python cli/cli_client.py

# Windows PowerShell
.\run_cli.ps1
```

**Interactive CLI Features:**
- ğŸ’¬ **Conversation Mode** - Maintains context across exchanges
- ğŸ”„ **Single-Prompt Mode** - Independent text generation
- âš™ï¸ **Adjustable Settings** - Temperature, top-k, max tokens
- ğŸ“ **Command System** - `/help`, `/settings`, `/clear`, etc.
- ğŸ’¾ **History Support** - Command history with readline

### 3. Train a Model

```bash
# Train the conversation model
python training/train_conversation.py
```

### 4. Generate Text

```bash
# Simple generation
python cli/generate.py --prompt "Hello world"

# Advanced generation with parameters
python cli/generate.py --prompt "Python is" --max_tokens 50 --temperature 0.8 --top_k 10

# From scripts directory
python scripts/main.py generate --prompt "Once upon a time"
```

### 5. Test the Implementation

```bash
# Basic functionality test
python tests/example.py

# Training test
python tests/test_train.py

# From root
python main.py test --type basic
```

## ğŸ’¾ Model Files

### **Active Models**
- **`pico_gpt_conversation.pt`** - **Primary conversation model**
  - 26.2M parameters (8 layers, 8 heads, 512 embedding dim)
  - **Default model** used by CLI
  - Optimized for natural conversations

- **`pico_gpt_large.pt`** - **Large capability model**
  - 88.9M parameters (12 layers, 12 heads, 768 embedding dim)
  - Maximum model capability
  - Use for complex tasks requiring more intelligence

### **Model Comparison**
| Model | Parameters | Size | Use Case |
|-------|------------|------|---------|
| `pico_gpt_conversation.pt` | 26.2M | ~100MB | ğŸŒŸ **Best for conversation** |
| `pico_gpt_large.pt` | 88.9M | ~350MB | Maximum capability, complex tasks |

## ğŸ¯ Command Line Interface

### Interactive CLI Commands

When in conversation mode, you can use these commands:

- `/help` - Show command reference
- `/settings` - View/modify generation parameters
- `/clear` - Clear screen
- `/reset` - Clear conversation context
- `/status` - Show conversation status
- `/mode` - Toggle conversation/single-prompt modes
- `/info` - Show model information
- `/load` - Load a different model
- `/quit` - Exit program

### Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--model` / `-m` | Path to model file | `pico_gpt_conversation.pt` |
| `--device` / `-d` | Device (cpu/cuda/auto) | `auto` |
| `--max-tokens` / `-t` | Maximum tokens to generate | `100` |
| `--temperature` / `-T` | Sampling temperature | `0.8` |
| `--top-k` / `-k` | Top-k sampling | `20` |
| `--prompt` / `-p` | Single prompt mode | None |

### Examples

```bash
# Interactive conversation
python cli/cli_client.py

# Windows PowerShell with custom settings
.\cli\run_cli.ps1 -Model "pico_gpt_large.pt" -MaxTokens 200 -Temperature 0.9

# Creative writing (high temperature)
python cli/generate.py --prompt "Once upon a time" --temperature 1.2 --max_tokens 200

# Factual completion (low temperature)
python cli/generate.py --prompt "Python is a programming language" --temperature 0.3

# Different model
python cli/cli_client.py --model models/pico_gpt_conversation.pt --prompt "Test conversation"
```

## ğŸ”§ Configuration

### Model Architecture

Edit the `GPTConfig` class in `src/pico_gpt.py` or create custom configs:

```python
from src.pico_gpt import GPTConfig

# Small model (fast training)
config = GPTConfig()
config.block_size = 256      # Context length
config.vocab_size = 50304    # Vocabulary size
config.n_layer = 6          # Number of transformer layers
config.n_head = 6           # Number of attention heads
config.n_embd = 384         # Embedding dimension
config.dropout = 0.2        # Dropout rate

# Tiny model (very fast)
config.n_layer = 2
config.n_head = 2
config.n_embd = 64
```

### Training Parameters

Modify training scripts for different setups:

```python
# Training hyperparameters
batch_size = 16             # Batch size
learning_rate = 3e-4        # Learning rate
max_iters = 5000           # Training iterations
eval_interval = 1000       # Validation frequency
```

## ğŸ“š Usage Examples

### Python API

```python
from src.pico_gpt import GPT, GPTConfig
from src.fast_tokenizer import GPT2LikeTokenizer
import torch

# Load model
checkpoint = torch.load('models/pico_gpt_conversation.pt', weights_only=False)
model = GPT(checkpoint['config'])
model.load_state_dict(checkpoint['model_state_dict'])
tokenizer = checkpoint['tokenizer']

# Generate text
model.eval()
context = torch.tensor(tokenizer.encode("Hello"), dtype=torch.long).unsqueeze(0)
generated = model.generate(context, max_new_tokens=100, temperature=0.8, top_k=10)
result = tokenizer.decode(generated[0].tolist())
print(result)
```

### Custom Training Data

```python
# training/train_custom.py
import torch
from src.pico_gpt import GPT, GPTConfig
from src.tokenizer import SimpleTokenizer

# Load your text data
with open('your_data.txt', 'r') as f:
    text = f.read()

# Create tokenizer and encode
tokenizer = SimpleTokenizer()
data = torch.tensor(tokenizer.encode(text), dtype=torch.long)

# Train model (see training scripts for full examples)
```

## âš¡ Performance Benchmarking

### CUDA vs CPU Performance

```bash
# Benchmark inference performance
python benchmarks/benchmark_large_model.py

# Training performance comparison
python benchmarks/benchmark_cuda_vs_cpu.py
```

**Example Results (RTX 3080 Ti):**
```
Large Model CUDA vs CPU Inference Benchmark
============================================================
Model: 25.7M parameters (8 layers, 8 heads, 512 embedding dim)

Average Generation Time:
  CPU:  2739.40 ms
  CUDA: 430.50 ms
  Speedup: 6.36x faster

Throughput (tokens/sec):
  CPU:  18.3
  CUDA: 116.1
  Improvement: 6.36x

Memory Usage:
  CUDA Allocated: 981.1 MB
```

## ğŸ“ Educational Notes

### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PICO GPT ARCHITECTURE                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                INPUT LAYER                              â”‚
â”‚                                                                         â”‚
â”‚  Input Text: "Hello world"                                              â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  â”‚   TOKENIZER     â”‚  Character/BPE tokenization                       â”‚
â”‚  â”‚  SimpleTokenizerâ”‚  "Hello world" â†’ [72, 101, 108, 108, 111, ...]    â”‚
â”‚  â”‚  BPETokenizer   â”‚                                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  Token IDs: [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           EMBEDDING LAYERS                              â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ Token Embedding  â”‚    â”‚Position Embeddingâ”‚                          â”‚
â”‚  â”‚    (wte)         â”‚    â”‚     (wpe)        â”‚                          â”‚
â”‚  â”‚  vocab_size      â”‚    â”‚   block_size     â”‚                          â”‚
â”‚  â”‚     â†“            â”‚    â”‚      â†“           â”‚                          â”‚
â”‚  â”‚  n_embd dim      â”‚    â”‚   n_embd dim     â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚            â”‚                       â”‚                                   â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                        â–¼                                               â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚                 â”‚  Element +  â”‚                                        â”‚
â”‚                 â”‚   Dropout   â”‚                                        â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                        â”‚                                               â”‚
â”‚                        â–¼                                               â”‚
â”‚              Embedded Sequence                                         â”‚
â”‚             [batch_size, seq_len, n_embd]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSFORMER BLOCKS (n_layer)                    â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€ BLOCK 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                    â”‚ â”‚
â”‚  â”‚  Input: x                                                          â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚ â”‚
â”‚  â”‚  â”‚ Layer Norm 1 â”‚                                                  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚ â”‚
â”‚  â”‚  â”‚         CAUSAL SELF-ATTENTION               â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚                                             â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Q,K,V   â”‚  Linear projection             â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Linear  â”‚  (n_embd â†’ 3 * n_embd)         â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚      â”‚                                      â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚      â–¼                                      â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚Multi-Headâ”‚  Split into n_head            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚Attention â”‚  Compute attention weights    â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚         â”‚  Apply causal mask             â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚      â”‚                                      â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚      â–¼                                      â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚Output   â”‚  Concatenate heads             â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚Linear   â”‚  Project back (n_embd)         â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚+Dropout â”‚                                â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚                  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â—„â”€â”€ Residual Connection                           â”‚ â”‚
â”‚  â”‚  â”‚    +     â”‚                                                      â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚ â”‚
â”‚  â”‚  â”‚ Layer Norm 2 â”‚                                                  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚ â”‚
â”‚  â”‚  â”‚                 MLP                          â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚                                              â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚   Linear    â”‚  n_embd â†’ 4 * n_embd       â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  (c_fc)     â”‚                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚         â”‚                                   â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚         â–¼                                   â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚    GELU     â”‚  Activation function       â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚         â”‚                                   â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚         â–¼                                   â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚   Linear    â”‚  4 * n_embd â†’ n_embd       â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ (c_proj)    â”‚                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  +Dropout   â”‚                            â”‚                  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚                  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â—„â”€â”€ Residual Connection                           â”‚ â”‚
â”‚  â”‚  â”‚    +     â”‚                                                      â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                      â”‚ â”‚
â”‚  â”‚     â”‚                                                              â”‚ â”‚
â”‚  â”‚     â–¼  Output to next block                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€ BLOCK 2...n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    (Same structure)                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FINAL OUTPUT LAYER                           â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚ Final Layer Norm â”‚  Normalize final transformer output              â”‚
â”‚  â”‚     (ln_f)       â”‚                                                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚           â”‚                                                            â”‚
â”‚           â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚ Language Model   â”‚  Linear: n_embd â†’ vocab_size                     â”‚
â”‚  â”‚ Head (lm_head)   â”‚  Weight tied with input embeddings              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚           â”‚                                                            â”‚
â”‚           â–¼                                                            â”‚
â”‚     Logits: [batch_size, seq_len, vocab_size]                         â”‚
â”‚                                                                         â”‚
â”‚  For Training:                    For Generation:                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Cross Entropy  â”‚               â”‚  Temperature    â”‚                  â”‚
â”‚  â”‚     Loss       â”‚               â”‚    Scaling      â”‚                  â”‚
â”‚  â”‚                â”‚               â”‚       â”‚         â”‚                  â”‚
â”‚  â”‚ Compare with   â”‚               â”‚       â–¼         â”‚                  â”‚
â”‚  â”‚ target tokens  â”‚               â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚ Top-k    â”‚   â”‚                  â”‚
â”‚                                   â”‚  â”‚Sampling  â”‚   â”‚                  â”‚
â”‚                                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                  â”‚
â”‚                                   â”‚       â”‚         â”‚                  â”‚
â”‚                                   â”‚       â–¼         â”‚                  â”‚
â”‚                                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                  â”‚
â”‚                                   â”‚  â”‚Multinomial   â”‚                  â”‚
â”‚                                   â”‚  â”‚ Sampling â”‚   â”‚                  â”‚
â”‚                                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                  â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                           â”‚                            â”‚
â”‚                                           â–¼                            â”‚
â”‚                                  Next Token Prediction                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            KEY COMPONENTS                               â”‚
â”‚                                                                         â”‚
â”‚ Configuration (GPTConfig):                                             â”‚
â”‚  â€¢ block_size: 1024 (max sequence length)                             â”‚
â”‚  â€¢ vocab_size: 50304 (vocabulary size)                                â”‚
â”‚  â€¢ n_layer: 12 (number of transformer blocks)                         â”‚
â”‚  â€¢ n_head: 12 (number of attention heads)                             â”‚
â”‚  â€¢ n_embd: 768 (embedding dimension)                                  â”‚
â”‚  â€¢ dropout: 0.0 (dropout rate)                                        â”‚
â”‚                                                                         â”‚
â”‚ Training Components:                                                    â”‚
â”‚  â€¢ Training Loop: batch processing, loss calculation                   â”‚
â”‚  â€¢ AdamW Optimizer: weight decay regularization                        â”‚
â”‚  â€¢ Learning Rate Scheduling: configurable learning rate               â”‚
â”‚  â€¢ Validation: periodic loss evaluation                                â”‚
â”‚  â€¢ Checkpointing: model state saving                                   â”‚
â”‚                                                                         â”‚
â”‚ Generation Features:                                                    â”‚
â”‚  â€¢ Autoregressive: generates one token at a time                       â”‚
â”‚  â€¢ Temperature Sampling: controls randomness                           â”‚
â”‚  â€¢ Top-k Sampling: limits vocabulary for coherent output               â”‚
â”‚  â€¢ Causal Masking: prevents looking ahead during attention             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            DATA FLOW SUMMARY                            â”‚
â”‚                                                                         â”‚
â”‚  Text Input â†’ Tokenization â†’ Embeddings â†’ Transformer Blocks â†’        â”‚
â”‚  â†’ Layer Norm â†’ Language Model Head â†’ Logits â†’ Sampling â†’ Output      â”‚
â”‚                                                                         â”‚
â”‚  Training: Input/Target pairs, Cross-entropy loss, Backpropagation     â”‚
â”‚  Generation: Autoregressive sampling with temperature and top-k        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Understanding the Architecture

1. **Transformer Blocks**: Each block contains self-attention + MLP
2. **Causal Attention**: Prevents looking at future tokens
3. **Position Embeddings**: Help model understand token order
4. **Layer Normalization**: Stabilizes training
5. **Weight Tying**: Input/output embeddings share weights

### Key Components

- `CausalSelfAttention`: Implements masked multi-head attention
- `MLP`: Feed-forward network with GELU activation
- `Block`: Complete transformer block
- `GPT`: Full model with embeddings and language modeling head

### Training Process

1. **Tokenization**: Convert text to integer sequences
2. **Batching**: Create input/target pairs
3. **Forward Pass**: Compute predictions and loss
4. **Backpropagation**: Update model weights
5. **Generation**: Sample from learned distribution

## âš¡ Performance Tips

### For Faster Training
- Reduce `n_layer`, `n_head`, `n_embd`
- Use smaller `block_size` and `batch_size`
- Enable GPU if available: `device = 'cuda'`

### For Better Generation
- Train longer (more iterations)
- Use larger vocabulary
- Increase model size
- Tune temperature and top_k

### Memory Optimization
- Reduce batch size if out of memory
- Use gradient checkpointing for large models
- Consider mixed precision training

## ğŸ› Troubleshooting

### Common Issues

**"CUDA out of memory"**
```bash
# Use CPU or reduce batch size
python cli/cli_client.py --device cpu
# Or reduce model size in config
```

**"Model file not found"**
```bash
# Train a model first
python training/train_small.py
```

**Poor generation quality**
```bash
# Train longer or with more data
# Use train_final.py for best conversation quality
```

**PowerShell Execution Policy (Windows)**
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

## âœ… Test Results

All core functionality has been verified:

### Verified Features
âœ… **Transformer Architecture** - Multi-head self-attention with causal masking  
âœ… **Training Pipeline** - Proper data batching, AdamW optimizer, checkpointing  
âœ… **Text Generation** - Autoregressive generation with temperature/top-k sampling  
âœ… **Tokenization** - Character-level and BPE tokenizers  
âœ… **CLI Interface** - Interactive conversation and single-prompt modes  
âœ… **GPU Support** - CUDA acceleration with automatic detection  

### Performance Characteristics
- **Training Speed**: 16 seconds for conversation model, ~143 tokens/second on CPU
- **Memory Usage**: Efficient for both small and large models
- **Convergence**: Good learning curves observed
- **Generation Quality**: Coherent outputs for trained models

## ğŸ“ˆ What's New (Post-Refactor)

### **Before Refactoring** âŒ
- Everything scattered in root folder
- Training scripts mixed with core code
- Hard to navigate and maintain
- Import path chaos

### **After Refactoring** âœ…
- **Clean structure**: Logical folder organization
- **Separated concerns**: Each folder has one purpose
- **Professional**: Industry-standard project layout
- **Maintainable**: Easy to find and modify code
- **Modular**: Components can be imported independently

## ğŸ“ Integration Examples

### Batch Scripts (Windows)
```batch
@echo off
python cli/cli_client.py --prompt "%1" --max-tokens 100 > output.txt
echo Generated text saved to output.txt
```

### PowerShell Functions
```powershell
function Generate-Text {
    param([string]$Prompt)
    python cli/cli_client.py --prompt $Prompt --max-tokens 100
}
```

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

---

*Happy training! ğŸš€*

**Ready for:**
- Educational purposes and learning transformers
- Experimentation with GPT architectures  
- Small-scale language modeling tasks
- Research and development
- Production use with proper scaling