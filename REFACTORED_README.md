# ğŸ¤– Pico-GPT - Refactored & Organized

A minimal, educational implementation of GPT (Generative Pre-trained Transformer) with **clean, professional project structure**.

## âœ… Refactored Project Structure

```
pico-gpt/
â”œâ”€â”€ ğŸ“ src/                      # Core implementation
â”‚   â”œâ”€â”€ pico_gpt.py             # Main GPT model & architecture
â”‚   â”œâ”€â”€ tokenizer.py            # Simple & BPE tokenizers
â”‚   â”œâ”€â”€ fast_tokenizer.py       # Optimized GPT-2 style tokenizer
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ training/                 # Training scripts
â”‚   â”œâ”€â”€ train_final.py          # ğŸŒŸ BEST: Fast conversation training
â”‚   â”œâ”€â”€ train_fast.py           # Speed-optimized training
â”‚   â”œâ”€â”€ train_conversation.py   # Conversation-focused training
â”‚   â”œâ”€â”€ train_large.py          # Large model training
â”‚   â”œâ”€â”€ train_small.py          # Quick testing
â”‚   â””â”€â”€ train.py                # Basic training
â”‚
â”œâ”€â”€ ğŸ“ cli/                      # User interfaces
â”‚   â”œâ”€â”€ cli_fast.py             # ğŸŒŸ MAIN: Interactive chat CLI
â”‚   â”œâ”€â”€ cli_client.py           # Alternative CLI
â”‚   â”œâ”€â”€ generate.py             # Simple text generation
â”‚   â””â”€â”€ run_cli.ps1             # Windows PowerShell launcher
â”‚
â”œâ”€â”€ ğŸ“ models/                   # Trained models
â”‚   â”œâ”€â”€ pico_gpt_final.pt       # ğŸŒŸ BEST: Fast conversation model
â”‚   â”œâ”€â”€ pico_gpt_fast.pt        # Speed-optimized model
â”‚   â”œâ”€â”€ pico_gpt_conversation.pt # Pure conversation model
â”‚   â””â”€â”€ [other model checkpoints...]
â”‚
â”œâ”€â”€ ğŸ“ datasets/                 # Training data & tokenizers
â”‚   â”œâ”€â”€ clean_conversation_data.txt    # ğŸŒŸ Clean chat data
â”‚   â”œâ”€â”€ conversation_data.txt          # Extended chat data
â”‚   â”œâ”€â”€ fast_tokenizer_gpt2_8000.pkl  # ğŸŒŸ Optimized tokenizer
â”‚   â””â”€â”€ [other datasets...]
â”‚
â”œâ”€â”€ ğŸ“ data/                     # Raw training data
â”‚   â”œâ”€â”€ combined_literature.txt  # Literature corpus
â”‚   â””â”€â”€ [classic books...]
â”‚
â”œâ”€â”€ ğŸ“ tests/                    # Test & example scripts
â”‚   â”œâ”€â”€ example.py              # Basic functionality demo
â”‚   â”œâ”€â”€ test_conversation.py    # Conversation testing
â”‚   â””â”€â”€ debug_conversation.py   # Debugging tools
â”‚
â”œâ”€â”€ ğŸ“ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ create_clean_conversation_data.py  # Generate chat data
â”‚   â””â”€â”€ create_conversation_data.py        # Generate extended data
â”‚
â”œâ”€â”€ ğŸ“ benchmarks/               # Performance testing
â”‚   â”œâ”€â”€ benchmark_large_model.py # CUDA vs CPU benchmarks
â”‚   â””â”€â”€ benchmark_cuda_vs_cpu.py # Training speed tests
â”‚
â”œâ”€â”€ ğŸ“„ main.py                   # ğŸŒŸ Main entry point
â”œâ”€â”€ ğŸ“„ run.py                    # Simple runner
â”œâ”€â”€ ğŸ“„ setup.py                  # Package installation
â”œâ”€â”€ ğŸ“„ requirements.txt          # Dependencies
â””â”€â”€ ğŸ“„ README.md                 # This file
```

## ğŸš€ Quick Start (Refactored)

### 1. **Chat with the Model** (Recommended)
```bash
# Using the simple runner
python run.py

# Or using the main entry point
python main.py chat

# Test with a specific prompt
python run.py --prompt "Hello, how are you?"
```

### 2. **Train a New Model**
```bash
# Train the best conversation model (16 seconds!)
cd training
python train_final.py

# Or from root
python main.py train --type conversation
```

### 3. **Generate Text**
```bash
cd cli
python cli_fast.py --prompt "Once upon a time"

# Or from root
python main.py generate --prompt "Hello world"
```

### 4. **Run Tests**
```bash
cd tests
python example.py

# Or from root  
python main.py test --type basic
```

## ğŸŒŸ Key Features After Refactoring

### **Optimized Models**
- **`models/pico_gpt_final.pt`** - 13.7M params, 16s training, conversation-focused
- **Ultra-fast training**: 23x faster than before
- **Proper conversation data**: No more literature regurgitation
- **Efficient tokenizer**: GPT-2 style with 4.27x compression

### **Professional Structure**
- âœ… **Organized by purpose**: src/, training/, cli/, tests/
- âœ… **Clean imports**: Proper Python package structure
- âœ… **Separated concerns**: Data, models, training, interfaces
- âœ… **Easy navigation**: No more scattered files

### **Multiple Interfaces**
- **CLI**: Interactive conversation mode
- **API**: Direct Python import from src/
- **Scripts**: Utility functions in scripts/
- **Main**: Unified entry point with subcommands

## ğŸ“Š Performance (Post-Refactor)

| Model | Size | Training Time | Use Case |
|-------|------|---------------|----------|
| `pico_gpt_final.pt` | 13.7M | 16 seconds | ğŸŒŸ **Best for chat** |
| `pico_gpt_fast.pt` | 13.7M | 72 seconds | Speed testing |
| `pico_gpt_conversation.pt` | 29.3M | 4 minutes | Extended conversations |

**CUDA Performance**: 2.5x speedup, 114MB memory usage

## ğŸ”§ Installation & Setup

```bash
# Clone and install
git clone <repo-url>
cd pico-gpt

# Install dependencies
pip install -r requirements.txt

# Or install as package
pip install -e .

# Quick test
python run.py --prompt "Hello!"
```

## ğŸ¯ What's Fixed

### **Before Refactoring** âŒ
- Everything scattered in root folder
- Training scripts mixed with core code
- Test files everywhere
- Import path chaos
- Hard to navigate

### **After Refactoring** âœ…
- **Clean structure**: Logical folder organization
- **Separated concerns**: Each folder has one purpose
- **Easy imports**: Proper Python package structure
- **Professional**: Industry-standard project layout
- **Maintainable**: Easy to find and modify code

## ğŸ“š Usage Examples

### **Quick Chat**
```bash
python run.py
# Starts interactive conversation mode
```

### **Training**
```bash
cd training
python train_final.py
# Trains conversation model in 16 seconds
```

### **Benchmarking**
```bash
cd benchmarks
python benchmark_large_model.py
# Tests CUDA vs CPU performance
```

### **Python API**
```python
from src.pico_gpt import GPT, GPTConfig
from src.fast_tokenizer import GPT2LikeTokenizer

# Load model
import torch
checkpoint = torch.load('models/pico_gpt_final.pt')
model = GPT(checkpoint['config'])
model.load_state_dict(checkpoint['model_state_dict'])
```

## ğŸ‰ Benefits of Refactoring

1. **ğŸ—‚ï¸ Organization**: Everything has its place
2. **ğŸ” Findability**: Easy to locate specific functionality  
3. **ğŸš€ Maintainability**: Changes are isolated and clear
4. **ğŸ“¦ Modularity**: Components can be imported independently
5. **ğŸ—ï¸ Professionalism**: Industry-standard project structure
6. **ğŸ§ª Testability**: Tests are separated and organized
7. **ğŸ“ˆ Scalability**: Easy to add new features

The codebase is now **production-ready** with proper organization!